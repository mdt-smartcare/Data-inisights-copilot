{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e901d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "527232e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 0. CUSTOM EMBEDDING CLASS ---\n",
    "# This custom class allows LangChain to use the local BGE-M3 model\n",
    "class LocalHuggingFaceEmbeddings(Embeddings):\n",
    "    def __init__(self, model_id):\n",
    "        self.model = SentenceTransformer(model_id)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        # Disable the model's internal progress bar to allow our main tqdm bar to control output\n",
    "        return self.model.encode(texts, show_progress_bar=False, normalize_embeddings=True).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text, normalize_embeddings=True).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b525415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to the Spice_BD database.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. DATABASE CONFIGURATION ---\n",
    "DB_USER = \"admin\"\n",
    "DB_PASSWORD = \"admin\"\n",
    "DB_NAME = \"Spice_BD\"  # MODIFIED: Changed to your database name\n",
    "DB_URI = f\"postgresql://{DB_USER}:{DB_PASSWORD}@localhost:5432/{DB_NAME}\"\n",
    "\n",
    "try:\n",
    "    engine = create_engine(DB_URI)\n",
    "    print(\"Successfully connected to the Spice_BD database.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to the database. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. SQL QUERIES FOR SPICE_BD SCHEMA (MODIFIED) ---\n",
    "# These queries are designed to fetch non-PII data for generating insights.\n",
    "sql_queries = {\n",
    "    \"patient_vitals\": \"\"\"\n",
    "        SELECT \n",
    "            pt.id AS patient_tracker_id,\n",
    "            p.gender, \n",
    "            p.age,\n",
    "            bp.avg_systolic, \n",
    "            bp.avg_diastolic, \n",
    "            gl.glucose_value, \n",
    "            gl.glucose_type\n",
    "        FROM patient_tracker pt\n",
    "        JOIN patient p ON pt.patient_id = p.id\n",
    "        LEFT JOIN bp_log bp ON pt.id = bp.patient_track_id\n",
    "        LEFT JOIN glucose_log gl ON pt.id = gl.patient_track_id\n",
    "        WHERE p.gender IS NOT NULL AND p.age IS NOT NULL;\n",
    "    \"\"\",\n",
    "    \"patient_conditions\": \"\"\"\n",
    "        SELECT \n",
    "            pt.id AS patient_tracker_id,\n",
    "            pd.is_htn_diagnosis, \n",
    "            pd.is_diabetes_diagnosis,\n",
    "            pc.comorbidity_id,\n",
    "            comp.complication_id\n",
    "        FROM patient_tracker pt\n",
    "        LEFT JOIN patient_diagnosis pd ON pt.id = pd.patient_track_id\n",
    "        LEFT JOIN patient_comorbidity pc ON pt.id = pc.patient_track_id\n",
    "        LEFT JOIN patient_complication comp ON pt.id = comp.patient_track_id;\n",
    "    \"\"\",\n",
    "    \"user_roles_permissions\": \"\"\"\n",
    "        SELECT\n",
    "            u.id AS user_id,\n",
    "            r.name AS role_name\n",
    "        FROM \"user\" u\n",
    "        JOIN user_role ur ON u.id = ur.user_id\n",
    "        JOIN role r ON ur.role_id = r.id;\n",
    "    \"\"\",\n",
    "    \"prescriptions\": \"\"\"\n",
    "        SELECT\n",
    "            p.patient_track_id,\n",
    "            p.medication_name,\n",
    "            p.prescribed_days\n",
    "        FROM prescription p;\n",
    "    \"\"\",\n",
    "    \"call_logs\": \"\"\"\n",
    "        SELECT\n",
    "            cr.id AS call_register_id,\n",
    "            cr.call_type,\n",
    "            crd.status AS call_status,\n",
    "            crd.duration AS call_duration\n",
    "        FROM call_register cr\n",
    "        JOIN call_register_detail crd ON cr.id = crd.call_register_id;\n",
    "    \"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31001ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 'patient_vitals'...\n",
      "Successfully loaded 9123 records for 'patient_vitals'.\n",
      "Loading data for 'patient_conditions'...\n",
      "Successfully loaded 8964 records for 'patient_conditions'.\n",
      "Loading data for 'user_roles_permissions'...\n",
      "Successfully loaded 1373 records for 'user_roles_permissions'.\n",
      "Loading data for 'prescriptions'...\n",
      "Successfully loaded 2533 records for 'prescriptions'.\n",
      "Loading data for 'call_logs'...\n",
      "Successfully loaded 1977 records for 'call_logs'.\n",
      "\n",
      "Processing loaded data for document creation...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. LOAD DATA FROM ALL TABLES ---\n",
    "dataframes = {}\n",
    "for name, query in sql_queries.items():\n",
    "    print(f\"Loading data for '{name}'...\")\n",
    "    try:\n",
    "        df = pd.read_sql(query, engine)\n",
    "        dataframes[name] = df\n",
    "        print(f\"Successfully loaded {len(df)} records for '{name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"--- QUERY FAILED for '{name}' ---\")\n",
    "        print(f\"The actual database error is: {e}\")\n",
    "\n",
    "# --- 4. DOCUMENT CREATION FOR SPICE_BD (MODIFIED) ---\n",
    "documents = []\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "print(\"\\nProcessing loaded data for document creation...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a4c66d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Vitals Documents: 100%|██████████| 9123/9123 [00:00<00:00, 47097.66it/s]\n",
      "Creating Conditions Documents: 100%|██████████| 8964/8964 [00:00<00:00, 54380.33it/s]\n",
      "Creating User Role Documents: 100%|██████████| 1373/1373 [00:00<00:00, 65997.15it/s]\n",
      "Creating Prescription Documents: 100%|██████████| 2533/2533 [00:00<00:00, 39427.06it/s]\n",
      "Creating Call Log Documents: 100%|██████████| 1977/1977 [00:00<00:00, 46037.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created a total of 23970 documents for indexing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Patient Vitals Documents ---\n",
    "if 'patient_vitals' in dataframes:\n",
    "    df = dataframes['patient_vitals']\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Creating Vitals Documents\"):\n",
    "        patient_id = row['patient_tracker_id']\n",
    "        content = (\n",
    "            f\"Patient-ID {patient_id} (Age: {row.get('age', 'N/A')}, Gender: {row.get('gender', 'N/A')}) has a recorded \"\n",
    "            f\"systolic pressure of {row.get('avg_systolic', 'N/A')}, \"\n",
    "            f\"diastolic pressure of {row.get('avg_diastolic', 'N/A')}, \"\n",
    "            f\"and a {row.get('glucose_type', 'N/A')} glucose value of {row.get('glucose_value', 'N/A')}.\"\n",
    "        )\n",
    "        doc = Document(page_content=content, metadata={\"source_patient_tracker_id\": patient_id, \"source_table\": \"patient_vitals\"})\n",
    "        documents.append(doc)\n",
    "\n",
    "# --- Patient Conditions Documents ---\n",
    "if 'patient_conditions' in dataframes:\n",
    "    df = dataframes['patient_conditions']\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Creating Conditions Documents\"):\n",
    "        patient_id = row['patient_tracker_id']\n",
    "        content = (\n",
    "            f\"Patient-ID {patient_id} has the following conditions: \"\n",
    "            f\"Hypertension Diagnosis: {row.get('is_htn_diagnosis', 'N/A')}, \"\n",
    "            f\"Diabetes Diagnosis: {row.get('is_diabetes_diagnosis', 'N/A')}, \"\n",
    "            f\"Comorbidity ID: {row.get('comorbidity_id', 'N/A')}, \"\n",
    "            f\"Complication ID: {row.get('complication_id', 'N/A')}.\"\n",
    "        )\n",
    "        doc = Document(page_content=content, metadata={\"source_patient_tracker_id\": patient_id, \"source_table\": \"patient_conditions\"})\n",
    "        documents.append(doc)\n",
    "\n",
    "# --- User Roles Documents ---\n",
    "if 'user_roles_permissions' in dataframes:\n",
    "    df = dataframes['user_roles_permissions']\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Creating User Role Documents\"):\n",
    "        user_id = row['user_id']\n",
    "        content = f\"User-ID {user_id} has the role of '{row.get('role_name', 'N/A')}'.\"\n",
    "        doc = Document(page_content=content, metadata={\"source_user_id\": user_id, \"source_table\": \"user_roles\"})\n",
    "        documents.append(doc)\n",
    "\n",
    "# --- Prescriptions Documents ---\n",
    "if 'prescriptions' in dataframes:\n",
    "    df = dataframes['prescriptions']\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Creating Prescription Documents\"):\n",
    "        text = f\"Medication '{row.get('medication_name', 'N/A')}' was prescribed for {row.get('prescribed_days', 'N/A')} days for Patient-ID {row.get('patient_track_id', 'N/A')}.\"\n",
    "        for chunk in text_splitter.split_text(text):\n",
    "            doc = Document(page_content=chunk, metadata={\"source_patient_tracker_id\": row.get('patient_track_id', 'N/A'), \"source_table\": \"prescription\"})\n",
    "            documents.append(doc)\n",
    "            \n",
    "# --- Call Logs Documents ---\n",
    "if 'call_logs' in dataframes:\n",
    "    df = dataframes['call_logs']\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Creating Call Log Documents\"):\n",
    "        content = f\"Call Log ID {row.get('call_register_id', 'N/A')}: Type of call was '{row.get('call_type', 'N/A')}' with a final status of '{row.get('call_status', 'N/A')}' and duration {row.get('call_duration', 'N/A')}.\"\n",
    "        doc = Document(page_content=content, metadata={\"source_call_id\": row.get('call_register_id', 'N/A'), \"source_table\": \"call_logs\"})\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f\"\\nCreated a total of {len(documents)} documents for indexing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa896bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BGE-M3 embedding model from local path...\n",
      "Model initialized.\n",
      "Creating and persisting the vector store (this may take a long time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding and Storing Batches: 100%|██████████| 188/188 [02:09<00:00,  1.45batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RAG Indexing Complete! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. EMBED AND STORE WITH BGE-M3 ---\n",
    "if documents:\n",
    "    print(\"Initializing BGE-M3 embedding model from local path...\")\n",
    "    \n",
    "    # Ensure you have cloned the model into this directory\n",
    "    model_path = \"./models/bge-m3\"  \n",
    "    \n",
    "    embedding_model = LocalHuggingFaceEmbeddings(model_id=model_path)\n",
    "    print(\"Model initialized.\")\n",
    "    persist_directory = 'chroma_db_spice_bd' # MODIFIED: New directory for this DB\n",
    "    \n",
    "    print(\"Creating and persisting the vector store (this may take a long time)...\")\n",
    "    \n",
    "    # Initialize Chroma with the first document to create the store\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=[documents[0]],\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    batch_size = 128 # Increased batch size for potentially faster processing\n",
    "    \n",
    "    # Loop through the rest of the documents in batches\n",
    "    for i in tqdm(range(1, len(documents), batch_size), \n",
    "                  desc=\"Embedding and Storing Batches\", \n",
    "                  unit=\"batch\"):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        if batch: # Ensure batch is not empty\n",
    "            vector_db.add_documents(documents=batch)\n",
    "\n",
    "    print(\"\\n--- RAG Indexing Complete! ---\")\n",
    "else:\n",
    "    print(\"\\nNo data was loaded or no documents were created. RAG indexing was skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e39109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model and vector store...\n",
      "Vector store loaded successfully.\n",
      "\n",
      "Performing similarity search for: 'any patients born after 2016??'\n",
      "\n",
      "--- Top 3 Retrieved Documents ---\n",
      "\n",
      "--- Document 1 ---\n",
      "Content: Patient-ID 20049 (Age: 27, Gender: Male) has a recorded systolic pressure of 150.0, diastolic pressure of 85.0, and a fbs glucose value of 8.0.\n",
      "Metadata: {'source_table': 'patient_vitals', 'source_patient_tracker_id': 20049}\n",
      "\n",
      "--- Document 2 ---\n",
      "Content: Patient-ID 21585 (Age: 45, Gender: Male) has a recorded systolic pressure of 150.0, diastolic pressure of 90.0, and a fbs glucose value of 9.0.\n",
      "Metadata: {'source_table': 'patient_vitals', 'source_patient_tracker_id': 21585}\n",
      "\n",
      "--- Document 3 ---\n",
      "Content: Patient-ID 20156 (Age: 45, Gender: Male) has a recorded systolic pressure of 150.0, diastolic pressure of 95.0, and a fbs glucose value of 9.0.\n",
      "Metadata: {'source_table': 'patient_vitals', 'source_patient_tracker_id': 20156}\n",
      "Checking for 'diabetes' in the source condition data...\n",
      "Could not find the 'condition' dataframe.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from typing import List\n",
    "\n",
    "# --- Make sure the custom embedding class is defined in your notebook ---\n",
    "class LocalHuggingFaceEmbeddings(Embeddings):\n",
    "    def __init__(self, model_id):\n",
    "        self.model = SentenceTransformer(model_id)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts, show_progress_bar=False).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text).tolist()\n",
    "\n",
    "# --- 1. SETTINGS ---\n",
    "model_path = \"./models/bge-m3\"\n",
    "persist_directory = 'chroma_db_spice_bd'  # Ensure this matches the directory used during indexing\n",
    "\n",
    "# --- 2. LOAD THE EMBEDDING MODEL AND VECTOR STORE ---\n",
    "print(\"Loading embedding model and vector store...\")\n",
    "embedding_model = LocalHuggingFaceEmbeddings(model_id=model_path)\n",
    "vector_db = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "print(\"Vector store loaded successfully.\")\n",
    "\n",
    "# --- 3. PERFORM A TEST QUERY ---\n",
    "query = \"any patients born after 2016??\"\n",
    "print(f\"\\nPerforming similarity search for: '{query}'\\n\")\n",
    "\n",
    "# Retrieve the 3 most relevant documents\n",
    "retrieved_docs = vector_db.similarity_search(query, k=3)\n",
    "\n",
    "# --- 4. DISPLAY THE RESULTS ---\n",
    "if retrieved_docs:\n",
    "    print(\"--- Top 3 Retrieved Documents ---\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"\\n--- Document {i+1} ---\")\n",
    "        print(f\"Content: {doc.page_content}\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "else:\n",
    "    print(\"No relevant documents were found.\")\n",
    "    \n",
    "# --- 5. VERIFY SOURCE DATA ---\n",
    "print(\"Checking for 'diabetes' in the source condition data...\")\n",
    "\n",
    "# Access the 'condition' dataframe from the dictionary we created earlier\n",
    "condition_df = dataframes.get('condition')\n",
    "\n",
    "if condition_df is not None:\n",
    "    # Search for rows where 'condition_text' contains 'Hypertension and Diabetics' (case-insensitive)\n",
    "    diabetes_records = condition_df[condition_df['condition_text'].str.contains('Hypertension and Diabetics', case=False, na=False)]\n",
    "\n",
    "\n",
    "    hypertension_records = condition_df[condition_df['condition_text'].str.contains('Hypertension', case=False, na=False)]\n",
    "    \n",
    "    if not diabetes_records.empty:\n",
    "        print(f\"\\nFound {len(diabetes_records)} records related to Hypertension and Diabetics in the source data.\")\n",
    "        print(\"Here are the first 3:\")\n",
    "        print(diabetes_records.head())\n",
    "    else:\n",
    "        print(\"This is likely why the similarity search did not return relevant results.\")\n",
    "    \n",
    "    if not hypertension_records.empty:\n",
    "        print(f\"\\nFound {len(hypertension_records)} records related to Hypertension in the source data.\")\n",
    "        print(\"Here are the first 3\")\n",
    "        print(hypertension_records.head())\n",
    "    else:\n",
    "        print(\"No records related to Hypertension were found.\")\n",
    "else:\n",
    "    print(\"Could not find the 'condition' dataframe.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
